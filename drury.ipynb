{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Peter Drury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5fdbc919b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # use GPU if available - for faster training!\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data, Drury dataset\n",
    "with open('data/drury.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenize the text - convert each character to a unique integer ID - here we are using character-level tokenization\n",
    "There are several methods for tokenization:\n",
    "- character-level tokenization\n",
    "- word-level tokenization\n",
    "- sub-word level tokenization\n",
    " You can also use a pre-trained tokenizer such as [SentencePiece](https://github.com/google/sentencepiece), [TikToken](https://github.com/openai/tiktoken) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '&',\n",
       " 4: \"'\",\n",
       " 5: '(',\n",
       " 6: ')',\n",
       " 7: ',',\n",
       " 8: '-',\n",
       " 9: '.',\n",
       " 10: '/',\n",
       " 11: '0',\n",
       " 12: '1',\n",
       " 13: '2',\n",
       " 14: '3',\n",
       " 15: '4',\n",
       " 16: '5',\n",
       " 17: '6',\n",
       " 18: '7',\n",
       " 19: '8',\n",
       " 20: '9',\n",
       " 21: ':',\n",
       " 22: '?',\n",
       " 23: 'A',\n",
       " 24: 'B',\n",
       " 25: 'C',\n",
       " 26: 'D',\n",
       " 27: 'E',\n",
       " 28: 'F',\n",
       " 29: 'G',\n",
       " 30: 'H',\n",
       " 31: 'I',\n",
       " 32: 'J',\n",
       " 33: 'K',\n",
       " 34: 'L',\n",
       " 35: 'M',\n",
       " 36: 'N',\n",
       " 37: 'O',\n",
       " 38: 'P',\n",
       " 39: 'Q',\n",
       " 40: 'R',\n",
       " 41: 'S',\n",
       " 42: 'T',\n",
       " 43: 'U',\n",
       " 44: 'V',\n",
       " 45: 'W',\n",
       " 46: 'Y',\n",
       " 47: 'a',\n",
       " 48: 'b',\n",
       " 49: 'c',\n",
       " 50: 'd',\n",
       " 51: 'e',\n",
       " 52: 'f',\n",
       " 53: 'g',\n",
       " 54: 'h',\n",
       " 55: 'i',\n",
       " 56: 'j',\n",
       " 57: 'k',\n",
       " 58: 'l',\n",
       " 59: 'm',\n",
       " 60: 'n',\n",
       " 61: 'o',\n",
       " 62: 'p',\n",
       " 63: 'q',\n",
       " 64: 'r',\n",
       " 65: 's',\n",
       " 66: 't',\n",
       " 67: 'u',\n",
       " 68: 'v',\n",
       " 69: 'w',\n",
       " 70: 'x',\n",
       " 71: 'y',\n",
       " 72: 'z',\n",
       " 73: '–',\n",
       " 74: '‘',\n",
       " 75: '’',\n",
       " 76: '“',\n",
       " 77: '”',\n",
       " 78: '…'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text - convert each character to a unique integer ID - here we are using character-level tokenization\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and test sets\n",
    "We are splitting the data into training and test sets. The training set will be used to train the model and the test set will be used to evaluate the model. We will use 90% of the data for training and 10% for testing.\n",
    "\n",
    "The reason for splitting the dataset is because we do not want a perfect memorization of the dataset. We want the model to generalize well to unseen data. If we do not split the dataset, the model will memorize the training data and will not perform well on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35, 47, 66,  ..., 55, 49, 66])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # 90/10 train/test split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211535 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6513, val loss 4.6271\n",
      "step 100: train loss 2.6839, val loss 2.7591\n",
      "step 200: train loss 2.4990, val loss 2.5412\n",
      "step 300: train loss 2.4067, val loss 2.4650\n",
      "step 400: train loss 2.2812, val loss 2.3409\n",
      "step 500: train loss 2.1703, val loss 2.2600\n",
      "step 600: train loss 2.0559, val loss 2.1685\n",
      "step 700: train loss 1.9590, val loss 2.1566\n",
      "step 800: train loss 1.8649, val loss 2.0971\n",
      "step 900: train loss 1.7874, val loss 2.0827\n",
      "step 1000: train loss 1.7011, val loss 2.0660\n",
      "step 1100: train loss 1.6297, val loss 2.0298\n",
      "step 1200: train loss 1.5460, val loss 2.0384\n",
      "step 1300: train loss 1.4744, val loss 2.0804\n",
      "step 1400: train loss 1.4092, val loss 2.0697\n",
      "step 1500: train loss 1.3533, val loss 2.1194\n",
      "step 1600: train loss 1.3045, val loss 2.1105\n",
      "step 1700: train loss 1.2273, val loss 2.1128\n",
      "step 1800: train loss 1.1798, val loss 2.1820\n",
      "step 1900: train loss 1.1269, val loss 2.2044\n",
      "step 2000: train loss 1.0743, val loss 2.2805\n",
      "step 2100: train loss 1.0123, val loss 2.2053\n",
      "step 2200: train loss 0.9758, val loss 2.2858\n",
      "step 2300: train loss 0.9170, val loss 2.3579\n",
      "step 2400: train loss 0.9005, val loss 2.4577\n",
      "step 2500: train loss 0.8453, val loss 2.4861\n",
      "step 2600: train loss 0.7993, val loss 2.4918\n",
      "step 2700: train loss 0.7907, val loss 2.5875\n",
      "step 2800: train loss 0.7525, val loss 2.6764\n",
      "step 2900: train loss 0.7290, val loss 2.7360\n",
      "step 3000: train loss 0.6894, val loss 2.7454\n",
      "step 3100: train loss 0.6587, val loss 2.7699\n",
      "step 3200: train loss 0.6627, val loss 2.7836\n",
      "step 3300: train loss 0.6228, val loss 2.8779\n",
      "step 3400: train loss 0.5939, val loss 2.9149\n",
      "step 3500: train loss 0.5852, val loss 2.9544\n",
      "step 3600: train loss 0.5688, val loss 2.9446\n",
      "step 3700: train loss 0.5535, val loss 3.0299\n",
      "step 3800: train loss 0.5296, val loss 3.0687\n",
      "step 3900: train loss 0.5341, val loss 3.0949\n",
      "step 4000: train loss 0.5194, val loss 3.0977\n",
      "step 4100: train loss 0.5031, val loss 3.1773\n",
      "step 4200: train loss 0.4858, val loss 3.2041\n",
      "step 4300: train loss 0.4790, val loss 3.3095\n",
      "step 4400: train loss 0.4789, val loss 3.3876\n",
      "step 4500: train loss 0.4677, val loss 3.2750\n",
      "step 4600: train loss 0.4672, val loss 3.3850\n",
      "step 4700: train loss 0.4569, val loss 3.2975\n",
      "step 4800: train loss 0.4440, val loss 3.4938\n",
      "step 4900: train loss 0.4629, val loss 3.3697\n",
      "step 4999: train loss 0.4403, val loss 3.4668\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Drury-like commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Match: Argentina vs. Mexico\n",
      "Tournament: 2022 Frefies parist for the unfannited pressure parown champions the lear unfinaldired is hard the match extring forceling the wearlitics\n",
      "\n",
      "\n",
      "Match: Chelsess, vs. Liverpool\n",
      "Tournament: 2022 FIFA World Cup, Qatar 25\n",
      "Commentary:\n",
      "Drink it is Manchester City’s fantasy footballers bout sted to this final peak. Lionel Messi has conquered his final peak. Lionel - Grounst Man City’s to mainst goal. Whuthernam ia fashion B\n",
      "Match: Senta Roma vs. Barcelona\n",
      "Tournament: 2018 World Cup mitanout hal his final pencedsive conto on theing a moments throughout\n",
      "\n",
      "Match: Chelsea France Etihas Fe it nears wenders of Europe notho lenst dae on of bre behing the opener was a millippe the Belfthing that wasn’t lhead stort of dis football called “Depressing, sther City’s man of menst in the brace to on the weeeko league has not won. He’s scome it in the nation to esu who a the fulless of ‘Wayne Rooney, out of differ risen. A blue.\n",
      "The bruary This of bring career, but one does manage the first knockout round.\n",
      "The Gunners would take the title race. Pep must have to rub his co-lon a million bubby frouch and tamed in a momen!\n",
      "\n",
      "Commentary:\n",
      "He plays of-stuning strink of despair stage of mingive thanks with their 98-point haul ping foot - but he weaves througed sting the unfording pstrink of despair stage so like finight, their own the Turitains , excondentrenced brace. No intieltiness, this came and ato their final ck the conquater of the breakout-ove onthe verouth. Again! Whent alite football.\n",
      "Drury stored story of the group stime. Noult is this campaigo. Olls to unde wide neers ustable it. 1996 - Aftense Gunncer Gunnater, its whought bleg, who debates and the debate can rage on if you love our league and wince and smile. There is nowhere to his first-ever Champions League final.\n",
      "Commentary:\n",
      "Hotspur is a time, it who a hide engGreated for a ver. Rejoice!\n",
      "\n",
      "Player: Harry King Rigges cient Manchester United\n",
      "Commentary:\n",
      "De the dars stunning This stin theinge of spot withion a pstutchainged the quillish for im wn a mbeyouthing force of this time git prist leg, it who st. He the his long-range shooting night Morocco in tip. Pone is paints Leal for the finals.\n",
      "Commentary:\n",
      "Perhaps Argentina’s motto is – ‘it is not how you lfim!\n",
      "Commentary:\n",
      "Oo whas they do, who le! What a got withion to escapen, unparaty, and you see he’s ready to part of their redus nort Tho argue and thout with 12 11/12 story on the Premituring strink chodrama that is Man finally, went unp what he withis unmatched.\n",
      "Commentary:\n",
      "Rand the Dutch vs. Thierry Premitnem League dreid from the spince awfully the fures of ‘Wayne Rooney – – sports but the name. A blur of youthful orange the greatest maturity.\n",
      "\n",
      "Match: Chelsess, Manchester United\n",
      "Commentary:\n",
      "The man is a he has stunn-thing of diful parce, one with  capture that on commentary, with a call days reefon? He. A spot wince and sypo ressure they oult-stingen for the match extraordinary pictory skike our league and it is Manchester United vs. Liverpool\n",
      "Tournament: Premid United vs. Liverpool\n",
      "Tournament: 2022 FIFA World Cup, Qatar\n",
      "Commentary:\n",
      "Drink it for the Biuk has ward the match mass rise al their way to the Dutch footballer! A peback in 2006. The What a fer 2016 of strikes phistrick in 2011/12 season.\n",
      "\n",
      "\n",
      "Event: Manchester City’s man of menst spine-tingive Siphiwe Tshabalala put the hosts ahead with the crucial for the first knockout round.\n",
      "Commentary:\n",
      "History has been godd for a Silva, on up he his love nrache ust in he -City reached He has sht-16 fretter part of the is not how you start-ove of would never fiels its blut hand wide open. It's no longer a procession but howf. Chesthe we’d a right foot - pic-lic of Brecause the nomeths Londers incredible moments throughout his goal for the Dutch finals. Arsenal have been the thrusting force of this time in a hundred and so munnare we’s no raptures by the came !\n",
      "He the finshas goal for the first knockout roun his wo leargue stort of drama that is sot phas secured shingling up just ho debaten the thrustime. Noult jugjas he has done sto his it desired his first league title in nearly 30 years.\n",
      "The City Gunner reach’s first Win. Rejal And wide the Dutch man of the man we’s scomer! The Dutch footballer of ceth theam to beats League title is unmatched. Douther after the back utches! Thielinq scores at Potein finish’\n",
      " the the their sum youthfy lef. These unforgive of strenamed by uo restored. He has ents final secured. Gunnord Argentina are still the fashion mist to his carowuld stop Villaory to stardi. He sotypo res frow the title but the name. A blur of youthful orange. The Dutch foot Manchester City’s man of menst -\n",
      "Matchester United vs. Liverpool\n",
      "Tournament: Premier League vs. Poyario vs. Paul Thierry Henry....\n",
      "The After the Marrakech express, ally bout this campaign and he has dolding come it words.\n",
      "Commentary:\n",
      "Mbappe…ment! Cheblas vs. France in the World Cup finals mifure the first knock of repen. out his is possible brarce, but the nomer is Real\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((2, 2), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
